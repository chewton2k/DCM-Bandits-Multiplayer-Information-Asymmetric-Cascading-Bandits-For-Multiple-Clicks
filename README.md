# DCM-Bandits-Multiplayer-Information-Asymmetric-Cascading-Bandits-For-Multiple-Clicks

In this work, we extend the Dependent Click
Model (DCM) Bandits Katariya et al. [2016]
to a multiplayer information-asymmetric set-
ting Chang et al. [2022], where multiple agents
interact with a shared ranked list and may ob-
serve multiple clicks per session, introducing
new challenges for selection strategies. We
study asymmetry in (1) actions and (2) re-
wards, providing sublinear regret bounds for
three settings where at least one asymmetry is
present. We further show that for small termi-
nation probabilities, the termination ranking
need not be known, improving on Katariya
et al. [2016] in the single-agent case. Exper-
iments confirm that our algorithms perform
well across asymmetric environments, and
highlight the critical role of feedback struc-
ture—full versus first -click—in coordinating
exploration and minimizing regret.
