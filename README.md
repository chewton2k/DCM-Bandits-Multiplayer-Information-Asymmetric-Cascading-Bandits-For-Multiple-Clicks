# DCM-Bandits-Multiplayer-Information-Asymmetric-Cascading-Bandits-For-Multiple-Clicks

Paper currently under review at TMLR.

In this work, we extend the Dependent Click Model (DCM) Bandits Katariya et al. [2016] to a multiplayer information-asymmetric setting Chang et al. [2022], where multiple agents interact with a shared ranked list and may observe multiple clicks per session, introducing new challenges for selection strategies. We study asymmetry in (1) actions and (2) rewards, providing sublinear regret bounds for three settings where at least one asymmetry is present. We further show that for small termination probabilities, the termination ranking need not be known, improving on Katariya et al. [2016] in the single-agent case. Experiments confirm that our algorithms perform well across asymmetric environments, and highlight the critical role of feedback struc- ture—full versus first -click—in coordinating exploration and minimizing regret.
